# -*- coding: utf-8 -*-
"""Final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/gist/sachinksachu/60b7a32046aea9dbb531326aa6ebc4f1/untitled5.ipynb
"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 1.4
import tensorflow
print(tensorflow.__version__)

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
df = pd.read_csv("/content/drive/My Drive/unbiased_project/train.csv")
df.head()

df = df.drop_duplicates(subset={'comment_text'},keep='first')
df.shape

# Ref: https://www.kaggle.com/haqishen/jigsaw-predict
# We are creating a dict with shortened word as key and actual word as value.
apostophe_dict = {"ain't": "is not", "aren't": "are not","can't": "cannot", "'cause": "because", "could've": "could have",
                  "couldn't": "could not", "didn't": "did not",  "doesn't": "does not", "don't": "do not", "hadn't": "had not",
                  "hasn't": "has not", "haven't": "have not", "he'd": "he would","he'll": "he will", "he's": "he is",
                  "how'd": "how did", "how'd'y": "how do you", "how'll": "how will", "how's": "how is",  "I'd": "I would",
                  "I'd've": "I would have", "I'll": "I will", "I'll've": "I will have","I'm": "I am", "I've": "I have",
                  "i'd": "i would", "i'd've": "i would have", "i'll": "i will",  "i'll've": "i will have","i'm": "i am",
                  "i've": "i have", "isn't": "is not", "it'd": "it would", "it'd've": "it would have", "it'll": "it will",
                  "it'll've": "it will have","it's": "it is", "let's": "let us", "ma'am": "madam", "mayn't": "may not",
                  "might've": "might have","mightn't": "might not","mightn't've": "might not have", "must've": "must have",
                  "mustn't": "must not", "mustn't've": "must not have", "needn't": "need not", "needn't've": "need not have",
                  "o'clock": "of the clock", "oughtn't": "ought not", "oughtn't've": "ought not have", "shan't": "shall not",
                  "sha'n't": "shall not", "shan't've": "shall not have", "she'd": "she would", "she'd've": "she would have",
                  "she'll": "she will", "she'll've": "she will have", "she's": "she is", "should've": "should have",
                  "shouldn't": "should not", "shouldn't've": "should not have", "so've": "so have","so's": "so as",
                  "this's": "this is","that'd": "that would", "that'd've": "that would have", "that's": "that is",
                  "there'd": "there would", "there'd've": "there would have", "there's": "there is", "here's": "here is",
                  "they'd": "they would", "they'd've": "they would have", "they'll": "they will", "they'll've": "they will have",
                  "they're": "they are", "they've": "they have", "to've": "to have", "wasn't": "was not", "we'd": "we would",
                  "we'd've": "we would have", "we'll": "we will", "we'll've": "we will have", "we're": "we are",
                  "we've": "we have", "weren't": "were not", "what'll": "what will", "what'll've": "what will have",
                  "what're": "what are",  "what's": "what is", "what've": "what have", "when's": "when is",
                  "when've": "when have", "where'd": "where did", "where's": "where is", "where've": "where have",
                  "who'll": "who will", "who'll've": "who will have", "who's": "who is", "who've": "who have",
                  "why's": "why is", "why've": "why have", "will've": "will have", "won't": "will not",
                  "won't've": "will not have", "would've": "would have", "wouldn't": "would not",
                  "wouldn't've": "would not have", "y'all": "you all", "y'all'd": "you all would",
                  "y'all'd've": "you all would have","y'all're": "you all are","y'all've": "you all have","you'd": "you would",
                  "you'd've": "you would have", "you'll": "you will", "you'll've": "you will have", "you're": "you are",
                  "you've": "you have" }

import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
stopwords = list(stopwords.words('english'))

import re

def process_sent(sent):
    line = ''
    for wrd in sent.split():
        if(len(apostophe_dict.get(wrd.lower(),'n'))>1 and wrd.lower() not in stopwords and len(wrd)>2):
            wrd = apostophe_dict[wrd.lower()]
            n_wrd = ''
            for w in wrd.split():
                if(w not in stopwords):
                    n_wrd += " "+w
            line += " "+n_wrd
        else:
            line += " "+ wrd.lower()    
    line = re.sub(r'[^a-zA-Z* ]',' ',line)
    return line

# Preprocessing the comments.
preprocessed_data = []
from tqdm import tqdm
for sent in tqdm(df['comment_text'].values):
    sent = process_sent(sent)
    line = ''
    for wrd in sent.split():
        if(len(wrd)>2):
            line += " " +wrd.lower()
    line = re.sub(r"[']",'',line)
    preprocessed_data.append(line)

df['comment_text'] = preprocessed_data
len(df)

from sklearn.model_selection import train_test_split
 
Y = df['target'].values
train,test,Y_train,Y_test = train_test_split(df,Y,test_size=0.1,random_state=42)
# train,cv,Y_train,Y_cv = train_test_split(train,Y_train,test_size=0.1,random_state=42)
print((train.shape),(Y_train.shape))

identity_columns = [
        'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',
        'muslim', 'black', 'white', 'psychiatric_or_mental_illness']

import numpy as np
seed = 1029

train_x = train['comment_text'].fillna('_##_').values
test_x = test['comment_text'].fillna('_##_').values
# cv_x = cv['comment_text'].fillna('_##_').values

# For gtrain
weights = np.ones((len(train),))
weights += train[identity_columns].fillna(0).values.sum(axis=1) * 3
weights += train['target'].values * (~train_x[identity_columns]).sum(axis=1)
weights /= weights.max()
train_y = np.vstack([train['target'], weights]).T   
train_y_identity = train[identity_columns].values

# For test
weights = np.ones((len(test),))
weights += test[identity_columns].fillna(0).values.sum(axis=1) * 3
weights += test['target'].values * (~train_x[identity_columns]).sum(axis=1)
weights /= weights.mean()
test_y = np.vstack([test['target'], weights]).T   
test_y_identity = test[identity_columns].values

np.random.seed(seed)
train_idx = np.random.permutation(len(train_x))
test_idx = np.random.permutation(len(test_x))

train_x = train_x[train_idx]
train_y = train_y[train_idx]
train_y_identity = train_y_identity[train_idx]


test_x = test_x[test_idx]
test_y = test_y[test_idx]
test_y_identity = test_y_identity[test_idx]

#y_binary = (test_y[:, 0] >= 0.5).astype(int)
#y_identity_binary = (test_y_identity >= 0.5).astype(int)
#y_identity_binary.shape

Y_train = (train_y[:,0]>=0.5).astype(int)
# Y_cv = (cv_y[:,0]>=0.5).astype(int)
Y_test = (test_y[:,0]>=0.5).astype(int)
# print(Y_train.shape, Y_cv.shape)

# target with value greater than equal to 0.5 will be assigned 1 and rest 0.
def fun(x):
    if(x>=0.5):
        return 1
    else:
        return 0

labels = df['target']
Y = labels.map(fun)

from sklearn.model_selection import train_test_split

X_train,X_test,Y_train,Y_test = train_test_split(preprocessed_data,Y,test_size=0.1,random_state=42)
# X_train,X_cv,Y_train,Y_cv = train_test_split(X_train,Y_train,test_size=0.1,random_state=42)
print(len(X_train),len(Y_train))

with open('/content/drive/My Drive/unbiased_project/newtok.txt', 'w') as filehandle:
    for listitem in train_x:
        filehandle.write('%s\n' % listitem)

f = open('/content/drive/My Drive/unbiased_project/HHH.txt', 'r')
newtok = f.readlines()
f.close()

# Tokenizing the essay text data for train dataset
# We will train the vectorizer on train data and will use the same on test and cv data.
# Credit : https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/
# Tokenizing the essay text data for train dataset
# We will train the vectorizer on train data and will use the same on test and cv data.
# Credit : https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.layers import Dense,Input
from numpy import asarray
t  = Tokenizer(num_words=40000)
t.fit_on_texts(X_train)
vocab_size = len(t.word_index)+1
# Integer coding all the words.
encoded_essay = t.texts_to_sequences(X_train)
# defining a max size for padding.
max_len = 150
# padding the vectors of each datapoint to fixed length of 600.
train_sequence = pad_sequences(encoded_essay,maxlen = max_len,padding='post')

# Vectorizing test data
# Integer coding all the words.
encoded_essay = t.texts_to_sequences(test_x)
# defining a max size for padding.
max_len = 150
# padding the vectors of each datapoint to fixed length of 600.
test_sequence = pad_sequences(encoded_essay,maxlen = max_len,padding='post')

import keras
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM, GRU
from keras.layers.embeddings import Embedding
from keras.preprocessing import sequence
from keras.layers import MaxPooling1D
from keras.layers import Flatten, GlobalMaxPooling1D, GlobalAveragePooling1D
from keras.layers import Dropout, BatchNormalization, Input, SpatialDropout1D
from keras.layers import Dense, Bidirectional, concatenate
from keras.models import Model
from keras.optimizers import Adam
# from keras_self_attention import SeqSelfAttention
import keras.backend as K

embedding_vector_length = 100
input1 = Input(shape=(150,))
e1 = Embedding(vocab_size,embedding_vector_length,input_length=150)(input1)
x1 =(LSTM(128, return_sequences=True, dropout=0.2)(e1)
# att = SeqSelfAttention(attention_activation='sigmoid')(x1)

x1 = Flatten()(x1)
output = Dense(1, activation='sigmoid')(x1)
model = Model(inputs=[input1,], outputs=[output])
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
model.summary()

model.fit(train_sequence, Y_train, nb_epoch=5, batch_size=512 , sample_weight=weights.values)
model.save('/content/drive/My Drive/unbiased_project/HHH.hdf5')









import keras
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM, GRU
from keras.layers.embeddings import Embedding
from keras.preprocessing import sequence
from keras.layers import MaxPooling1D
from keras.layers import Flatten, GlobalMaxPooling1D, GlobalAveragePooling1D
from keras.layers import Dropout, BatchNormalization, Input, SpatialDropout1D
from keras.layers import Dense, Bidirectional, concatenate
from keras.models import Model
from keras.optimizers import Adam
# from keras_self_attention import SeqSelfAttention
import keras.backend as K

f = open('/content/drive/My Drive/unbiased_project/HHH.txt', 'r')
HHH = f.readlines()
f.close()

from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.layers import Dense,Input
from numpy import asarray
t  = Tokenizer(num_words=40000)
# t.fit_on_texts(X_train)
t.fit_on_texts(HHH)

from keras.models import load_model
model_dwn = load_model('/content/drive/My Drive/unbiased_project/HHH.hdf5')

sample_texts = ["you are man"]
encoded_sample = t.texts_to_sequences(sample_texts)
# defining a max size for padding.
max_len = 150
# padding the vectors of each datapoint to fixed length of 600.
pad_sample = pad_sequences(encoded_sample,maxlen = max_len,padding='post')
preds = model_dwn.predict(pad_sample)
print(preds)
